@article{berner2019dota,
  title         = {Dota 2 with Large Scale Deep Reinforcement Learning},
  author        = {Berner, Christopher and Brockman, Greg and Chan, Brooke and Cheung, Vicki and Debiak, Przemys\l{}aw and Dennison, Christy and Farhi, David and Fischer, Quirin and Hashme, Shariq and Hesse, Chris and J\'{o}zefowicz, Rafal and Gray, Scott and Olsson, Catherine and Pachocki, Jakub and Petrov, Michael and Pinto, Henrique and Raiman, Jonathan and Salimans, Tim and Schlatter, Jeremy and Zhang, Susan},
  year          = {2019},
  month         = {12},
  journal       = {arXiv preprint arXiv:1912.06680}
}
@inproceedings{rudin2022learning,
  title         = {Learning to walk in minutes using massively parallel deep reinforcement learning},
  author        = {Rudin, Nikita and Hoeller, David and Reist, Philipp and Hutter, Marco},
  booktitle     = {Conference on Robot Learning},
  pages         = {91--100},
  year          = {2022},
  organization  = {PMLR}
}
@article{makoviychuk2021isaac,
  title         = {Isaac gym: High performance gpu-based physics simulation for robot learning},
  author        = {Makoviychuk, Viktor and Wawrzyniak, Lukasz and Guo, Yunrong and Lu, Michelle and Storey, Kier and Macklin, Miles and Hoeller, David and Rudin, Nikita and Allshire, Arthur and Handa, Ankur and others},
  journal       = {arXiv preprint arXiv:2108.10470},
  year          = {2021}
}
@article{huang2023openrlbenchmark,
  title         = {Open RL Benchmark: Comprehensive Tracked Experiments for Reinforcement Learning},
  author        = {Huang, Shengyi and Gallou{\'e}dec, Quentin and Felten, Florian and Raffin, Antonin and Dossa, Rousslan Fernand Julien and Zhao, Yanxiao and Sullivan, Ryan and Makoviychuk, Viktor and Makoviichuk, Denys and Danesh, Mohamad H and others},
  journal       = {arXiv preprint arXiv:2402.03046},
  year          = {2024}
}
@article{raffin2021sb3,
  author        = {Antonin Raffin and Ashley Hill and Adam Gleave and Anssi Kanervisto and Maximilian Ernestus and Noah Dormann},
  title         = {Stable-Baselines3: Reliable Reinforcement Learning Implementations},
  journal       = {Journal of Machine Learning Research},
  year          = {2021},
  volume        = {22},
  number        = {268},
  pages         = {1--8}
}
@inproceedings{kuznetsov2020tqc,
  author        = {Kuznetsov, Arsenii and Shvechikov, Pavel and Grishin, Alexander and Vetrov, Dmitry},
  title         = {Controlling Overestimation Bias with Truncated Mixture of Continuous Distributional Quantile Critics},
  year          = {2020},
  publisher     = {JMLR.org},
  booktitle     = {Proceedings of the 37th International Conference on Machine Learning},
  articleno     = {515},
  numpages      = {11},
  series        = {ICML'20}
}
@inproceedings{haarnoja2018soft,
  title         = {Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor},
  author        = {Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  pages         = {1861--1870},
  year          = {2018},
  month         = jul,
  booktitle     = {International Conference on Machine Learning}
}
@misc{schulman2017proximal,
  title         = {Proximal policy optimization algorithms},
  author        = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  year          = {2017},
  eprint        = {1707.06347},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  howpublished  = {arXiv:1707.06347v2 [cs.LG]}
}
@inproceedings{shengyi2022the37implementation,
  author        = {Huang, Shengyi and Dossa, Rousslan Fernand Julien and Raffin, Antonin and Kanervisto, Anssi and Wang, Weixun},
  title         = {The 37 Implementation Details of Proximal Policy Optimization},
  booktitle     = {ICLR Blog Track},
  year          = {2022},
  url           = {https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/}
}
